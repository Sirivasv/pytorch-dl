{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/shakespeare.txt', 'r', encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose mi\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5445609"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '>',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '|',\n",
       " '}'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\n',\n",
       " 1: ' ',\n",
       " 2: '!',\n",
       " 3: '\"',\n",
       " 4: '&',\n",
       " 5: \"'\",\n",
       " 6: '(',\n",
       " 7: ')',\n",
       " 8: ',',\n",
       " 9: '-',\n",
       " 10: '.',\n",
       " 11: '0',\n",
       " 12: '1',\n",
       " 13: '2',\n",
       " 14: '3',\n",
       " 15: '4',\n",
       " 16: '5',\n",
       " 17: '6',\n",
       " 18: '7',\n",
       " 19: '8',\n",
       " 20: '9',\n",
       " 21: ':',\n",
       " 22: ';',\n",
       " 23: '<',\n",
       " 24: '>',\n",
       " 25: '?',\n",
       " 26: 'A',\n",
       " 27: 'B',\n",
       " 28: 'C',\n",
       " 29: 'D',\n",
       " 30: 'E',\n",
       " 31: 'F',\n",
       " 32: 'G',\n",
       " 33: 'H',\n",
       " 34: 'I',\n",
       " 35: 'J',\n",
       " 36: 'K',\n",
       " 37: 'L',\n",
       " 38: 'M',\n",
       " 39: 'N',\n",
       " 40: 'O',\n",
       " 41: 'P',\n",
       " 42: 'Q',\n",
       " 43: 'R',\n",
       " 44: 'S',\n",
       " 45: 'T',\n",
       " 46: 'U',\n",
       " 47: 'V',\n",
       " 48: 'W',\n",
       " 49: 'X',\n",
       " 50: 'Y',\n",
       " 51: 'Z',\n",
       " 52: '[',\n",
       " 53: ']',\n",
       " 54: '_',\n",
       " 55: '`',\n",
       " 56: 'a',\n",
       " 57: 'b',\n",
       " 58: 'c',\n",
       " 59: 'd',\n",
       " 60: 'e',\n",
       " 61: 'f',\n",
       " 62: 'g',\n",
       " 63: 'h',\n",
       " 64: 'i',\n",
       " 65: 'j',\n",
       " 66: 'k',\n",
       " 67: 'l',\n",
       " 68: 'm',\n",
       " 69: 'n',\n",
       " 70: 'o',\n",
       " 71: 'p',\n",
       " 72: 'q',\n",
       " 73: 'r',\n",
       " 74: 's',\n",
       " 75: 't',\n",
       " 76: 'u',\n",
       " 77: 'v',\n",
       " 78: 'w',\n",
       " 79: 'x',\n",
       " 80: 'y',\n",
       " 81: 'z',\n",
       " 82: '|',\n",
       " 83: '}'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num --> letter\n",
    "decoder = dict(enumerate(sorted(all_characters)))\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# letter --> num\n",
    "encoder = {char: ind for ind, char in decoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '\"': 3,\n",
       " '&': 4,\n",
       " \"'\": 5,\n",
       " '(': 6,\n",
       " ')': 7,\n",
       " ',': 8,\n",
       " '-': 9,\n",
       " '.': 10,\n",
       " '0': 11,\n",
       " '1': 12,\n",
       " '2': 13,\n",
       " '3': 14,\n",
       " '4': 15,\n",
       " '5': 16,\n",
       " '6': 17,\n",
       " '7': 18,\n",
       " '8': 19,\n",
       " '9': 20,\n",
       " ':': 21,\n",
       " ';': 22,\n",
       " '<': 23,\n",
       " '>': 24,\n",
       " '?': 25,\n",
       " 'A': 26,\n",
       " 'B': 27,\n",
       " 'C': 28,\n",
       " 'D': 29,\n",
       " 'E': 30,\n",
       " 'F': 31,\n",
       " 'G': 32,\n",
       " 'H': 33,\n",
       " 'I': 34,\n",
       " 'J': 35,\n",
       " 'K': 36,\n",
       " 'L': 37,\n",
       " 'M': 38,\n",
       " 'N': 39,\n",
       " 'O': 40,\n",
       " 'P': 41,\n",
       " 'Q': 42,\n",
       " 'R': 43,\n",
       " 'S': 44,\n",
       " 'T': 45,\n",
       " 'U': 46,\n",
       " 'V': 47,\n",
       " 'W': 48,\n",
       " 'X': 49,\n",
       " 'Y': 50,\n",
       " 'Z': 51,\n",
       " '[': 52,\n",
       " ']': 53,\n",
       " '_': 54,\n",
       " '`': 55,\n",
       " 'a': 56,\n",
       " 'b': 57,\n",
       " 'c': 58,\n",
       " 'd': 59,\n",
       " 'e': 60,\n",
       " 'f': 61,\n",
       " 'g': 62,\n",
       " 'h': 63,\n",
       " 'i': 64,\n",
       " 'j': 65,\n",
       " 'k': 66,\n",
       " 'l': 67,\n",
       " 'm': 68,\n",
       " 'n': 69,\n",
       " 'o': 70,\n",
       " 'p': 71,\n",
       " 'q': 72,\n",
       " 'r': 73,\n",
       " 's': 74,\n",
       " 't': 75,\n",
       " 'u': 76,\n",
       " 'v': 77,\n",
       " 'w': 78,\n",
       " 'x': 79,\n",
       " 'y': 80,\n",
       " 'z': 81,\n",
       " '|': 82,\n",
       " '}': 83}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1, 12,  0,  1,  1, 31, 73, 70, 68,  1, 61, 56, 64,\n",
       "       73, 60, 74, 75,  1, 58, 73, 60, 56, 75, 76, 73, 60, 74,  1, 78])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(encoded_text, num_uni_chars):\n",
    "    \n",
    "    # encoded_text --> batch of encoded text\n",
    "    # num_uni_chars --> len(set(text))\n",
    "    \n",
    "    one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
    "    \n",
    "    one_hot = one_hot.astype(np.float32)\n",
    "    \n",
    "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()]  = 1.0\n",
    "    \n",
    "    one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([1, 2, 0])\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder(arr, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5],\n",
       "       [6, 7],\n",
       "       [8, 9]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text.reshape((5,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n",
    "    \n",
    "    # X : encoded text of legth seq_len\n",
    "    # Y : encoded text shifted by one\n",
    "    \n",
    "    # how many chars per batch?\n",
    "    char_per_batch = samp_per_batch * seq_len\n",
    "    \n",
    "    # how many batches can we make, given the len of the encoded text?\n",
    "    num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
    "    \n",
    "    # Cut off the end of the encoded text, that won't fit evenly into a batch\n",
    "    encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n",
    "\n",
    "    encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
    "    \n",
    "    for n in range(0, encoded_text.shape[1], seq_len):\n",
    "        \n",
    "        x = encoded_text[:, n:n+seq_len]\n",
    "        \n",
    "        # zeros array to the same shape as x\n",
    "        y = np.zeros_like(x)\n",
    "        \n",
    "        try:\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1] = encoded_text[:, n+seq_len]\n",
    "        except:\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:,-1] = encoded_text[:, 0]\n",
    "        \n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = encoded_text[:20]\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_generator = generate_batches(sample_text, samp_per_batch=2, seq_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, all_chars, num_hidden=256, num_layers=4, drop_prob=0.5, use_gpu=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        self.all_chars = all_chars\n",
    "        self.decoder = dict(enumerate(sorted(all_chars)))\n",
    "        self.encoder = {char: ind for ind, char in decoder.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        lstm_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        drop_output = self.dropout(lstm_output)\n",
    "        \n",
    "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "        \n",
    "        final_out = self.fc_linear(drop_output)\n",
    "        \n",
    "        return final_out, hidden\n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            \n",
    "            hidden = (torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda(), torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda())\n",
    "        else:\n",
    "            hidden = (torch.zeros(self.num_layers, batch_size, self.num_hidden), torch.zeros(self.num_layers, batch_size, self.num_hidden))\n",
    "        \n",
    "        return hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharModel(all_chars=all_characters,\n",
    "                  num_hidden=512,\n",
    "                  num_layers=3,\n",
    "                  drop_prob=0.5,\n",
    "                  use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5470292"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_param = []\n",
    "\n",
    "for p in model.parameters():\n",
    "    total_param.append(int(p.numel()))\n",
    "\n",
    "sum(total_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4901048, 544561)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_percent = 0.9\n",
    "train_ind = int(len(encoded_text)*train_percent)\n",
    "\n",
    "train_data = encoded_text[:train_ind]\n",
    "val_data = encoded_text[train_ind:]\n",
    "\n",
    "len(train_data), len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES\n",
    "\n",
    "epochs = 25\n",
    "batch_size = 128\n",
    "\n",
    "seq_len = 100\n",
    "\n",
    "tracker = 0\n",
    "num_char = max(encoded_text) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 step: 25 VAL LOSS: 3.2180914878845215\n",
      "EPOCH: 0 step: 50 VAL LOSS: 3.2096810340881348\n",
      "EPOCH: 0 step: 75 VAL LOSS: 3.1886563301086426\n",
      "EPOCH: 0 step: 100 VAL LOSS: 3.0191195011138916\n",
      "EPOCH: 0 step: 125 VAL LOSS: 2.897557258605957\n",
      "EPOCH: 0 step: 150 VAL LOSS: 2.740764856338501\n",
      "EPOCH: 0 step: 175 VAL LOSS: 2.639618158340454\n",
      "EPOCH: 0 step: 200 VAL LOSS: 2.519855260848999\n",
      "EPOCH: 0 step: 225 VAL LOSS: 2.4080257415771484\n",
      "EPOCH: 0 step: 250 VAL LOSS: 2.3026647567749023\n",
      "EPOCH: 0 step: 275 VAL LOSS: 2.206455945968628\n",
      "EPOCH: 0 step: 300 VAL LOSS: 2.1456210613250732\n",
      "EPOCH: 0 step: 325 VAL LOSS: 2.1011931896209717\n",
      "EPOCH: 0 step: 350 VAL LOSS: 2.0498414039611816\n",
      "EPOCH: 0 step: 375 VAL LOSS: 2.009038209915161\n",
      "EPOCH: 1 step: 400 VAL LOSS: 1.971071720123291\n",
      "EPOCH: 1 step: 425 VAL LOSS: 1.9333325624465942\n",
      "EPOCH: 1 step: 450 VAL LOSS: 1.8990142345428467\n",
      "EPOCH: 1 step: 475 VAL LOSS: 1.8732837438583374\n",
      "EPOCH: 1 step: 500 VAL LOSS: 1.8417056798934937\n",
      "EPOCH: 1 step: 525 VAL LOSS: 1.8174331188201904\n",
      "EPOCH: 1 step: 550 VAL LOSS: 1.7950385808944702\n",
      "EPOCH: 1 step: 575 VAL LOSS: 1.7811424732208252\n",
      "EPOCH: 1 step: 600 VAL LOSS: 1.7527905702590942\n",
      "EPOCH: 1 step: 625 VAL LOSS: 1.7335293292999268\n",
      "EPOCH: 1 step: 650 VAL LOSS: 1.7189955711364746\n",
      "EPOCH: 1 step: 675 VAL LOSS: 1.6970298290252686\n",
      "EPOCH: 1 step: 700 VAL LOSS: 1.6789259910583496\n",
      "EPOCH: 1 step: 725 VAL LOSS: 1.6669288873672485\n",
      "EPOCH: 1 step: 750 VAL LOSS: 1.6501063108444214\n",
      "EPOCH: 2 step: 775 VAL LOSS: 1.6463615894317627\n",
      "EPOCH: 2 step: 800 VAL LOSS: 1.625919222831726\n",
      "EPOCH: 2 step: 825 VAL LOSS: 1.6130599975585938\n",
      "EPOCH: 2 step: 850 VAL LOSS: 1.601388692855835\n",
      "EPOCH: 2 step: 875 VAL LOSS: 1.5907574892044067\n",
      "EPOCH: 2 step: 900 VAL LOSS: 1.5824432373046875\n",
      "EPOCH: 2 step: 925 VAL LOSS: 1.5677412748336792\n",
      "EPOCH: 2 step: 950 VAL LOSS: 1.5551036596298218\n",
      "EPOCH: 2 step: 975 VAL LOSS: 1.549180269241333\n",
      "EPOCH: 2 step: 1000 VAL LOSS: 1.5391887426376343\n",
      "EPOCH: 2 step: 1025 VAL LOSS: 1.530476689338684\n",
      "EPOCH: 2 step: 1050 VAL LOSS: 1.5263804197311401\n",
      "EPOCH: 2 step: 1075 VAL LOSS: 1.5228434801101685\n",
      "EPOCH: 2 step: 1100 VAL LOSS: 1.5103732347488403\n",
      "EPOCH: 2 step: 1125 VAL LOSS: 1.5058499574661255\n",
      "EPOCH: 3 step: 1150 VAL LOSS: 1.5073188543319702\n",
      "EPOCH: 3 step: 1175 VAL LOSS: 1.5034866333007812\n",
      "EPOCH: 3 step: 1200 VAL LOSS: 1.4947919845581055\n",
      "EPOCH: 3 step: 1225 VAL LOSS: 1.4904817342758179\n",
      "EPOCH: 3 step: 1250 VAL LOSS: 1.4802274703979492\n",
      "EPOCH: 3 step: 1275 VAL LOSS: 1.4749860763549805\n",
      "EPOCH: 3 step: 1300 VAL LOSS: 1.4693068265914917\n",
      "EPOCH: 3 step: 1325 VAL LOSS: 1.4619674682617188\n",
      "EPOCH: 3 step: 1350 VAL LOSS: 1.4626719951629639\n",
      "EPOCH: 3 step: 1375 VAL LOSS: 1.4582703113555908\n",
      "EPOCH: 3 step: 1400 VAL LOSS: 1.4506022930145264\n",
      "EPOCH: 3 step: 1425 VAL LOSS: 1.4440454244613647\n",
      "EPOCH: 3 step: 1450 VAL LOSS: 1.4414968490600586\n",
      "EPOCH: 3 step: 1475 VAL LOSS: 1.4381840229034424\n",
      "EPOCH: 3 step: 1500 VAL LOSS: 1.4384675025939941\n",
      "EPOCH: 3 step: 1525 VAL LOSS: 1.4354437589645386\n",
      "EPOCH: 4 step: 1550 VAL LOSS: 1.427312970161438\n",
      "EPOCH: 4 step: 1575 VAL LOSS: 1.4255067110061646\n",
      "EPOCH: 4 step: 1600 VAL LOSS: 1.4230248928070068\n",
      "EPOCH: 4 step: 1625 VAL LOSS: 1.41806960105896\n",
      "EPOCH: 4 step: 1650 VAL LOSS: 1.4189114570617676\n",
      "EPOCH: 4 step: 1675 VAL LOSS: 1.4140795469284058\n",
      "EPOCH: 4 step: 1700 VAL LOSS: 1.4073699712753296\n",
      "EPOCH: 4 step: 1725 VAL LOSS: 1.4081376791000366\n",
      "EPOCH: 4 step: 1750 VAL LOSS: 1.4036141633987427\n",
      "EPOCH: 4 step: 1775 VAL LOSS: 1.4003512859344482\n",
      "EPOCH: 4 step: 1800 VAL LOSS: 1.3987035751342773\n",
      "EPOCH: 4 step: 1825 VAL LOSS: 1.3961666822433472\n",
      "EPOCH: 4 step: 1850 VAL LOSS: 1.3943922519683838\n",
      "EPOCH: 4 step: 1875 VAL LOSS: 1.3943564891815186\n",
      "EPOCH: 4 step: 1900 VAL LOSS: 1.3896636962890625\n",
      "EPOCH: 5 step: 1925 VAL LOSS: 1.387416124343872\n",
      "EPOCH: 5 step: 1950 VAL LOSS: 1.3877224922180176\n",
      "EPOCH: 5 step: 1975 VAL LOSS: 1.3873755931854248\n",
      "EPOCH: 5 step: 2000 VAL LOSS: 1.3797886371612549\n",
      "EPOCH: 5 step: 2025 VAL LOSS: 1.381690502166748\n",
      "EPOCH: 5 step: 2050 VAL LOSS: 1.3829646110534668\n",
      "EPOCH: 5 step: 2075 VAL LOSS: 1.3752219676971436\n",
      "EPOCH: 5 step: 2100 VAL LOSS: 1.3712917566299438\n",
      "EPOCH: 5 step: 2125 VAL LOSS: 1.3748396635055542\n",
      "EPOCH: 5 step: 2150 VAL LOSS: 1.3680016994476318\n",
      "EPOCH: 5 step: 2175 VAL LOSS: 1.3717530965805054\n",
      "EPOCH: 5 step: 2200 VAL LOSS: 1.3732942342758179\n",
      "EPOCH: 5 step: 2225 VAL LOSS: 1.3668429851531982\n",
      "EPOCH: 5 step: 2250 VAL LOSS: 1.3675342798233032\n",
      "EPOCH: 5 step: 2275 VAL LOSS: 1.3670941591262817\n",
      "EPOCH: 6 step: 2300 VAL LOSS: 1.3635200262069702\n",
      "EPOCH: 6 step: 2325 VAL LOSS: 1.3702548742294312\n",
      "EPOCH: 6 step: 2350 VAL LOSS: 1.3629379272460938\n",
      "EPOCH: 6 step: 2375 VAL LOSS: 1.3596066236495972\n",
      "EPOCH: 6 step: 2400 VAL LOSS: 1.3599809408187866\n",
      "EPOCH: 6 step: 2425 VAL LOSS: 1.3563034534454346\n",
      "EPOCH: 6 step: 2450 VAL LOSS: 1.3546947240829468\n",
      "EPOCH: 6 step: 2475 VAL LOSS: 1.3526703119277954\n",
      "EPOCH: 6 step: 2500 VAL LOSS: 1.353061318397522\n",
      "EPOCH: 6 step: 2525 VAL LOSS: 1.3496273756027222\n",
      "EPOCH: 6 step: 2550 VAL LOSS: 1.347127079963684\n",
      "EPOCH: 6 step: 2575 VAL LOSS: 1.3502079248428345\n",
      "EPOCH: 6 step: 2600 VAL LOSS: 1.348808765411377\n",
      "EPOCH: 6 step: 2625 VAL LOSS: 1.3489800691604614\n",
      "EPOCH: 6 step: 2650 VAL LOSS: 1.3511757850646973\n",
      "EPOCH: 7 step: 2675 VAL LOSS: 1.344588279724121\n",
      "EPOCH: 7 step: 2700 VAL LOSS: 1.3380517959594727\n",
      "EPOCH: 7 step: 2725 VAL LOSS: 1.3448611497879028\n",
      "EPOCH: 7 step: 2750 VAL LOSS: 1.3381222486495972\n",
      "EPOCH: 7 step: 2775 VAL LOSS: 1.3369899988174438\n",
      "EPOCH: 7 step: 2800 VAL LOSS: 1.335904598236084\n",
      "EPOCH: 7 step: 2825 VAL LOSS: 1.336369276046753\n",
      "EPOCH: 7 step: 2850 VAL LOSS: 1.3343147039413452\n",
      "EPOCH: 7 step: 2875 VAL LOSS: 1.3345626592636108\n",
      "EPOCH: 7 step: 2900 VAL LOSS: 1.332413911819458\n",
      "EPOCH: 7 step: 2925 VAL LOSS: 1.3275647163391113\n",
      "EPOCH: 7 step: 2950 VAL LOSS: 1.332494854927063\n",
      "EPOCH: 7 step: 2975 VAL LOSS: 1.333686113357544\n",
      "EPOCH: 7 step: 3000 VAL LOSS: 1.328669786453247\n",
      "EPOCH: 7 step: 3025 VAL LOSS: 1.3369134664535522\n",
      "EPOCH: 7 step: 3050 VAL LOSS: 1.3261451721191406\n",
      "EPOCH: 8 step: 3075 VAL LOSS: 1.3286951780319214\n",
      "EPOCH: 8 step: 3100 VAL LOSS: 1.3240890502929688\n",
      "EPOCH: 8 step: 3125 VAL LOSS: 1.3253871202468872\n",
      "EPOCH: 8 step: 3150 VAL LOSS: 1.3210231065750122\n",
      "EPOCH: 8 step: 3175 VAL LOSS: 1.319994330406189\n",
      "EPOCH: 8 step: 3200 VAL LOSS: 1.3219115734100342\n",
      "EPOCH: 8 step: 3225 VAL LOSS: 1.3207621574401855\n",
      "EPOCH: 8 step: 3250 VAL LOSS: 1.3215157985687256\n",
      "EPOCH: 8 step: 3275 VAL LOSS: 1.3221415281295776\n",
      "EPOCH: 8 step: 3300 VAL LOSS: 1.3184216022491455\n",
      "EPOCH: 8 step: 3325 VAL LOSS: 1.3209538459777832\n",
      "EPOCH: 8 step: 3350 VAL LOSS: 1.321239948272705\n",
      "EPOCH: 8 step: 3375 VAL LOSS: 1.3175510168075562\n",
      "EPOCH: 8 step: 3400 VAL LOSS: 1.3265153169631958\n",
      "EPOCH: 8 step: 3425 VAL LOSS: 1.3196039199829102\n",
      "EPOCH: 9 step: 3450 VAL LOSS: 1.3134989738464355\n",
      "EPOCH: 9 step: 3475 VAL LOSS: 1.3138004541397095\n",
      "EPOCH: 9 step: 3500 VAL LOSS: 1.3172578811645508\n",
      "EPOCH: 9 step: 3525 VAL LOSS: 1.3098143339157104\n",
      "EPOCH: 9 step: 3550 VAL LOSS: 1.314297080039978\n",
      "EPOCH: 9 step: 3575 VAL LOSS: 1.3104478120803833\n",
      "EPOCH: 9 step: 3600 VAL LOSS: 1.3076177835464478\n",
      "EPOCH: 9 step: 3625 VAL LOSS: 1.3099784851074219\n",
      "EPOCH: 9 step: 3650 VAL LOSS: 1.3147146701812744\n",
      "EPOCH: 9 step: 3675 VAL LOSS: 1.308674931526184\n",
      "EPOCH: 9 step: 3700 VAL LOSS: 1.3073517084121704\n",
      "EPOCH: 9 step: 3725 VAL LOSS: 1.311173439025879\n",
      "EPOCH: 9 step: 3750 VAL LOSS: 1.3078134059906006\n",
      "EPOCH: 9 step: 3775 VAL LOSS: 1.313915729522705\n",
      "EPOCH: 9 step: 3800 VAL LOSS: 1.3076649904251099\n",
      "EPOCH: 10 step: 3825 VAL LOSS: 1.3087886571884155\n",
      "EPOCH: 10 step: 3850 VAL LOSS: 1.3017489910125732\n",
      "EPOCH: 10 step: 3875 VAL LOSS: 1.3088868856430054\n",
      "EPOCH: 10 step: 3900 VAL LOSS: 1.3056122064590454\n",
      "EPOCH: 10 step: 3925 VAL LOSS: 1.3024581670761108\n",
      "EPOCH: 10 step: 3950 VAL LOSS: 1.3004924058914185\n",
      "EPOCH: 10 step: 3975 VAL LOSS: 1.3012213706970215\n",
      "EPOCH: 10 step: 4000 VAL LOSS: 1.3058754205703735\n",
      "EPOCH: 10 step: 4025 VAL LOSS: 1.3020753860473633\n",
      "EPOCH: 10 step: 4050 VAL LOSS: 1.3003050088882446\n",
      "EPOCH: 10 step: 4075 VAL LOSS: 1.3035712242126465\n",
      "EPOCH: 10 step: 4100 VAL LOSS: 1.2995920181274414\n",
      "EPOCH: 10 step: 4125 VAL LOSS: 1.3026646375656128\n",
      "EPOCH: 10 step: 4150 VAL LOSS: 1.3000893592834473\n",
      "EPOCH: 10 step: 4175 VAL LOSS: 1.298817753791809\n",
      "EPOCH: 10 step: 4200 VAL LOSS: 1.3015211820602417\n",
      "EPOCH: 11 step: 4225 VAL LOSS: 1.2974706888198853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 11 step: 4250 VAL LOSS: 1.3011677265167236\n",
      "EPOCH: 11 step: 4275 VAL LOSS: 1.298019289970398\n",
      "EPOCH: 11 step: 4300 VAL LOSS: 1.2975560426712036\n",
      "EPOCH: 11 step: 4325 VAL LOSS: 1.2978390455245972\n",
      "EPOCH: 11 step: 4350 VAL LOSS: 1.2937487363815308\n",
      "EPOCH: 11 step: 4375 VAL LOSS: 1.2943077087402344\n",
      "EPOCH: 11 step: 4400 VAL LOSS: 1.2942732572555542\n",
      "EPOCH: 11 step: 4425 VAL LOSS: 1.2905383110046387\n",
      "EPOCH: 11 step: 4450 VAL LOSS: 1.291762351989746\n",
      "EPOCH: 11 step: 4475 VAL LOSS: 1.2929528951644897\n",
      "EPOCH: 11 step: 4500 VAL LOSS: 1.2927238941192627\n",
      "EPOCH: 11 step: 4525 VAL LOSS: 1.289785623550415\n",
      "EPOCH: 11 step: 4550 VAL LOSS: 1.292944312095642\n",
      "EPOCH: 11 step: 4575 VAL LOSS: 1.2892264127731323\n",
      "EPOCH: 12 step: 4600 VAL LOSS: 1.2875159978866577\n",
      "EPOCH: 12 step: 4625 VAL LOSS: 1.2877768278121948\n",
      "EPOCH: 12 step: 4650 VAL LOSS: 1.2913166284561157\n",
      "EPOCH: 12 step: 4675 VAL LOSS: 1.286894679069519\n",
      "EPOCH: 12 step: 4700 VAL LOSS: 1.2913082838058472\n",
      "EPOCH: 12 step: 4725 VAL LOSS: 1.2887665033340454\n",
      "EPOCH: 12 step: 4750 VAL LOSS: 1.2910014390945435\n",
      "EPOCH: 12 step: 4775 VAL LOSS: 1.2886385917663574\n",
      "EPOCH: 12 step: 4800 VAL LOSS: 1.2881072759628296\n",
      "EPOCH: 12 step: 4825 VAL LOSS: 1.2891087532043457\n",
      "EPOCH: 12 step: 4850 VAL LOSS: 1.2871044874191284\n",
      "EPOCH: 12 step: 4875 VAL LOSS: 1.2883960008621216\n",
      "EPOCH: 12 step: 4900 VAL LOSS: 1.2858703136444092\n",
      "EPOCH: 12 step: 4925 VAL LOSS: 1.2861779928207397\n",
      "EPOCH: 12 step: 4950 VAL LOSS: 1.2851911783218384\n",
      "EPOCH: 13 step: 4975 VAL LOSS: 1.286110281944275\n",
      "EPOCH: 13 step: 5000 VAL LOSS: 1.2833900451660156\n",
      "EPOCH: 13 step: 5025 VAL LOSS: 1.2875394821166992\n",
      "EPOCH: 13 step: 5050 VAL LOSS: 1.2843496799468994\n",
      "EPOCH: 13 step: 5075 VAL LOSS: 1.2874655723571777\n",
      "EPOCH: 13 step: 5100 VAL LOSS: 1.2877328395843506\n",
      "EPOCH: 13 step: 5125 VAL LOSS: 1.2861636877059937\n",
      "EPOCH: 13 step: 5150 VAL LOSS: 1.2845333814620972\n",
      "EPOCH: 13 step: 5175 VAL LOSS: 1.2886357307434082\n",
      "EPOCH: 13 step: 5200 VAL LOSS: 1.2862902879714966\n",
      "EPOCH: 13 step: 5225 VAL LOSS: 1.290428876876831\n",
      "EPOCH: 13 step: 5250 VAL LOSS: 1.2872214317321777\n",
      "EPOCH: 13 step: 5275 VAL LOSS: 1.2857455015182495\n",
      "EPOCH: 13 step: 5300 VAL LOSS: 1.2832449674606323\n",
      "EPOCH: 13 step: 5325 VAL LOSS: 1.2851839065551758\n",
      "EPOCH: 14 step: 5350 VAL LOSS: 1.2867547273635864\n",
      "EPOCH: 14 step: 5375 VAL LOSS: 1.2819204330444336\n",
      "EPOCH: 14 step: 5400 VAL LOSS: 1.2901616096496582\n",
      "EPOCH: 14 step: 5425 VAL LOSS: 1.2816340923309326\n",
      "EPOCH: 14 step: 5450 VAL LOSS: 1.2794976234436035\n",
      "EPOCH: 14 step: 5475 VAL LOSS: 1.2813724279403687\n",
      "EPOCH: 14 step: 5500 VAL LOSS: 1.2812973260879517\n",
      "EPOCH: 14 step: 5525 VAL LOSS: 1.2807929515838623\n",
      "EPOCH: 14 step: 5550 VAL LOSS: 1.2818716764450073\n",
      "EPOCH: 14 step: 5575 VAL LOSS: 1.281213402748108\n",
      "EPOCH: 14 step: 5600 VAL LOSS: 1.2822238206863403\n",
      "EPOCH: 14 step: 5625 VAL LOSS: 1.281313419342041\n",
      "EPOCH: 14 step: 5650 VAL LOSS: 1.2819212675094604\n",
      "EPOCH: 14 step: 5675 VAL LOSS: 1.2775391340255737\n",
      "EPOCH: 14 step: 5700 VAL LOSS: 1.2805644273757935\n",
      "EPOCH: 14 step: 5725 VAL LOSS: 1.2813278436660767\n",
      "EPOCH: 15 step: 5750 VAL LOSS: 1.2785468101501465\n",
      "EPOCH: 15 step: 5775 VAL LOSS: 1.2789510488510132\n",
      "EPOCH: 15 step: 5800 VAL LOSS: 1.2784502506256104\n",
      "EPOCH: 15 step: 5825 VAL LOSS: 1.2767212390899658\n",
      "EPOCH: 15 step: 5850 VAL LOSS: 1.2790977954864502\n",
      "EPOCH: 15 step: 5875 VAL LOSS: 1.2787654399871826\n",
      "EPOCH: 15 step: 5900 VAL LOSS: 1.279371976852417\n",
      "EPOCH: 15 step: 5925 VAL LOSS: 1.2763705253601074\n",
      "EPOCH: 15 step: 5950 VAL LOSS: 1.2797521352767944\n",
      "EPOCH: 15 step: 5975 VAL LOSS: 1.2796926498413086\n",
      "EPOCH: 15 step: 6000 VAL LOSS: 1.2836707830429077\n",
      "EPOCH: 15 step: 6025 VAL LOSS: 1.2806522846221924\n",
      "EPOCH: 15 step: 6050 VAL LOSS: 1.2753390073776245\n",
      "EPOCH: 15 step: 6075 VAL LOSS: 1.2771861553192139\n",
      "EPOCH: 15 step: 6100 VAL LOSS: 1.276252269744873\n",
      "EPOCH: 16 step: 6125 VAL LOSS: 1.2801294326782227\n",
      "EPOCH: 16 step: 6150 VAL LOSS: 1.278717041015625\n",
      "EPOCH: 16 step: 6175 VAL LOSS: 1.282502293586731\n",
      "EPOCH: 16 step: 6200 VAL LOSS: 1.2757424116134644\n",
      "EPOCH: 16 step: 6225 VAL LOSS: 1.2773699760437012\n",
      "EPOCH: 16 step: 6250 VAL LOSS: 1.2761130332946777\n",
      "EPOCH: 16 step: 6275 VAL LOSS: 1.2772324085235596\n",
      "EPOCH: 16 step: 6300 VAL LOSS: 1.2789278030395508\n",
      "EPOCH: 16 step: 6325 VAL LOSS: 1.2786551713943481\n",
      "EPOCH: 16 step: 6350 VAL LOSS: 1.2761374711990356\n",
      "EPOCH: 16 step: 6375 VAL LOSS: 1.2784444093704224\n",
      "EPOCH: 16 step: 6400 VAL LOSS: 1.274251103401184\n",
      "EPOCH: 16 step: 6425 VAL LOSS: 1.2725833654403687\n",
      "EPOCH: 16 step: 6450 VAL LOSS: 1.2781398296356201\n",
      "EPOCH: 16 step: 6475 VAL LOSS: 1.2761387825012207\n",
      "EPOCH: 17 step: 6500 VAL LOSS: 1.2777656316757202\n",
      "EPOCH: 17 step: 6525 VAL LOSS: 1.2747687101364136\n",
      "EPOCH: 17 step: 6550 VAL LOSS: 1.276289939880371\n",
      "EPOCH: 17 step: 6575 VAL LOSS: 1.2746357917785645\n",
      "EPOCH: 17 step: 6600 VAL LOSS: 1.2733253240585327\n",
      "EPOCH: 17 step: 6625 VAL LOSS: 1.2736868858337402\n",
      "EPOCH: 17 step: 6650 VAL LOSS: 1.2750571966171265\n",
      "EPOCH: 17 step: 6675 VAL LOSS: 1.2823318243026733\n",
      "EPOCH: 17 step: 6700 VAL LOSS: 1.2804979085922241\n",
      "EPOCH: 17 step: 6725 VAL LOSS: 1.2747478485107422\n",
      "EPOCH: 17 step: 6750 VAL LOSS: 1.2748210430145264\n",
      "EPOCH: 17 step: 6775 VAL LOSS: 1.2726566791534424\n",
      "EPOCH: 17 step: 6800 VAL LOSS: 1.2766438722610474\n",
      "EPOCH: 17 step: 6825 VAL LOSS: 1.2717355489730835\n",
      "EPOCH: 17 step: 6850 VAL LOSS: 1.2730193138122559\n",
      "EPOCH: 17 step: 6875 VAL LOSS: 1.2741657495498657\n",
      "EPOCH: 18 step: 6900 VAL LOSS: 1.2699761390686035\n",
      "EPOCH: 18 step: 6925 VAL LOSS: 1.2719558477401733\n",
      "EPOCH: 18 step: 6950 VAL LOSS: 1.272566795349121\n",
      "EPOCH: 18 step: 6975 VAL LOSS: 1.2743170261383057\n",
      "EPOCH: 18 step: 7000 VAL LOSS: 1.272895336151123\n",
      "EPOCH: 18 step: 7025 VAL LOSS: 1.2737191915512085\n",
      "EPOCH: 18 step: 7050 VAL LOSS: 1.2759356498718262\n",
      "EPOCH: 18 step: 7075 VAL LOSS: 1.273432970046997\n",
      "EPOCH: 18 step: 7100 VAL LOSS: 1.2788243293762207\n",
      "EPOCH: 18 step: 7125 VAL LOSS: 1.272355079650879\n",
      "EPOCH: 18 step: 7150 VAL LOSS: 1.2723251581192017\n",
      "EPOCH: 18 step: 7175 VAL LOSS: 1.2766032218933105\n",
      "EPOCH: 18 step: 7200 VAL LOSS: 1.2724206447601318\n",
      "EPOCH: 18 step: 7225 VAL LOSS: 1.275031566619873\n",
      "EPOCH: 18 step: 7250 VAL LOSS: 1.2661689519882202\n",
      "EPOCH: 19 step: 7275 VAL LOSS: 1.2695473432540894\n",
      "EPOCH: 19 step: 7300 VAL LOSS: 1.2712074518203735\n",
      "EPOCH: 19 step: 7325 VAL LOSS: 1.2723315954208374\n",
      "EPOCH: 19 step: 7350 VAL LOSS: 1.2703142166137695\n",
      "EPOCH: 19 step: 7375 VAL LOSS: 1.2734793424606323\n",
      "EPOCH: 19 step: 7400 VAL LOSS: 1.2708450555801392\n",
      "EPOCH: 19 step: 7425 VAL LOSS: 1.272128939628601\n",
      "EPOCH: 19 step: 7450 VAL LOSS: 1.2704771757125854\n",
      "EPOCH: 19 step: 7475 VAL LOSS: 1.272606611251831\n",
      "EPOCH: 19 step: 7500 VAL LOSS: 1.2695326805114746\n",
      "EPOCH: 19 step: 7525 VAL LOSS: 1.2719999551773071\n",
      "EPOCH: 19 step: 7550 VAL LOSS: 1.2712479829788208\n",
      "EPOCH: 19 step: 7575 VAL LOSS: 1.2682595252990723\n",
      "EPOCH: 19 step: 7600 VAL LOSS: 1.2684460878372192\n",
      "EPOCH: 19 step: 7625 VAL LOSS: 1.2711496353149414\n",
      "EPOCH: 20 step: 7650 VAL LOSS: 1.269881248474121\n",
      "EPOCH: 20 step: 7675 VAL LOSS: 1.2665807008743286\n",
      "EPOCH: 20 step: 7700 VAL LOSS: 1.268967866897583\n",
      "EPOCH: 20 step: 7725 VAL LOSS: 1.2647109031677246\n",
      "EPOCH: 20 step: 7750 VAL LOSS: 1.2684547901153564\n",
      "EPOCH: 20 step: 7775 VAL LOSS: 1.2661343812942505\n",
      "EPOCH: 20 step: 7800 VAL LOSS: 1.270153284072876\n",
      "EPOCH: 20 step: 7825 VAL LOSS: 1.265993595123291\n",
      "EPOCH: 20 step: 7850 VAL LOSS: 1.2700982093811035\n",
      "EPOCH: 20 step: 7875 VAL LOSS: 1.268762230873108\n",
      "EPOCH: 20 step: 7900 VAL LOSS: 1.2717148065567017\n",
      "EPOCH: 20 step: 7925 VAL LOSS: 1.2679831981658936\n",
      "EPOCH: 20 step: 7950 VAL LOSS: 1.2683916091918945\n",
      "EPOCH: 20 step: 7975 VAL LOSS: 1.2716087102890015\n",
      "EPOCH: 20 step: 8000 VAL LOSS: 1.2729907035827637\n",
      "EPOCH: 21 step: 8025 VAL LOSS: 1.2742395401000977\n",
      "EPOCH: 21 step: 8050 VAL LOSS: 1.2681351900100708\n",
      "EPOCH: 21 step: 8075 VAL LOSS: 1.272826910018921\n",
      "EPOCH: 21 step: 8100 VAL LOSS: 1.2693378925323486\n",
      "EPOCH: 21 step: 8125 VAL LOSS: 1.270295262336731\n",
      "EPOCH: 21 step: 8150 VAL LOSS: 1.2688965797424316\n",
      "EPOCH: 21 step: 8175 VAL LOSS: 1.2661734819412231\n",
      "EPOCH: 21 step: 8200 VAL LOSS: 1.2745438814163208\n",
      "EPOCH: 21 step: 8225 VAL LOSS: 1.2735817432403564\n",
      "EPOCH: 21 step: 8250 VAL LOSS: 1.2690789699554443\n",
      "EPOCH: 21 step: 8275 VAL LOSS: 1.2681605815887451\n",
      "EPOCH: 21 step: 8300 VAL LOSS: 1.2673205137252808\n",
      "EPOCH: 21 step: 8325 VAL LOSS: 1.2681630849838257\n",
      "EPOCH: 21 step: 8350 VAL LOSS: 1.262039303779602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 21 step: 8375 VAL LOSS: 1.2652943134307861\n",
      "EPOCH: 21 step: 8400 VAL LOSS: 1.2643505334854126\n",
      "EPOCH: 22 step: 8425 VAL LOSS: 1.2666525840759277\n",
      "EPOCH: 22 step: 8450 VAL LOSS: 1.2651171684265137\n",
      "EPOCH: 22 step: 8475 VAL LOSS: 1.267947793006897\n",
      "EPOCH: 22 step: 8500 VAL LOSS: 1.268045425415039\n",
      "EPOCH: 22 step: 8525 VAL LOSS: 1.2681775093078613\n",
      "EPOCH: 22 step: 8550 VAL LOSS: 1.2701382637023926\n",
      "EPOCH: 22 step: 8575 VAL LOSS: 1.2689363956451416\n",
      "EPOCH: 22 step: 8600 VAL LOSS: 1.2675384283065796\n",
      "EPOCH: 22 step: 8625 VAL LOSS: 1.267837643623352\n",
      "EPOCH: 22 step: 8650 VAL LOSS: 1.2710185050964355\n",
      "EPOCH: 22 step: 8675 VAL LOSS: 1.273576021194458\n",
      "EPOCH: 22 step: 8700 VAL LOSS: 1.2695801258087158\n",
      "EPOCH: 22 step: 8725 VAL LOSS: 1.2641892433166504\n",
      "EPOCH: 22 step: 8750 VAL LOSS: 1.2642698287963867\n",
      "EPOCH: 22 step: 8775 VAL LOSS: 1.2648593187332153\n",
      "EPOCH: 23 step: 8800 VAL LOSS: 1.2665349245071411\n",
      "EPOCH: 23 step: 8825 VAL LOSS: 1.2676286697387695\n",
      "EPOCH: 23 step: 8850 VAL LOSS: 1.267871618270874\n",
      "EPOCH: 23 step: 8875 VAL LOSS: 1.2686078548431396\n",
      "EPOCH: 23 step: 8900 VAL LOSS: 1.2690986394882202\n",
      "EPOCH: 23 step: 8925 VAL LOSS: 1.2645279169082642\n",
      "EPOCH: 23 step: 8950 VAL LOSS: 1.2667049169540405\n",
      "EPOCH: 23 step: 8975 VAL LOSS: 1.2652342319488525\n",
      "EPOCH: 23 step: 9000 VAL LOSS: 1.2686017751693726\n",
      "EPOCH: 23 step: 9025 VAL LOSS: 1.2656060457229614\n",
      "EPOCH: 23 step: 9050 VAL LOSS: 1.2691371440887451\n",
      "EPOCH: 23 step: 9075 VAL LOSS: 1.2676154375076294\n",
      "EPOCH: 23 step: 9100 VAL LOSS: 1.264133095741272\n",
      "EPOCH: 23 step: 9125 VAL LOSS: 1.2684407234191895\n",
      "EPOCH: 23 step: 9150 VAL LOSS: 1.2696545124053955\n",
      "EPOCH: 24 step: 9175 VAL LOSS: 1.2767400741577148\n",
      "EPOCH: 24 step: 9200 VAL LOSS: 1.2643992900848389\n",
      "EPOCH: 24 step: 9225 VAL LOSS: 1.2720847129821777\n",
      "EPOCH: 24 step: 9250 VAL LOSS: 1.2703826427459717\n",
      "EPOCH: 24 step: 9275 VAL LOSS: 1.2640128135681152\n",
      "EPOCH: 24 step: 9300 VAL LOSS: 1.2642030715942383\n",
      "EPOCH: 24 step: 9325 VAL LOSS: 1.2658991813659668\n",
      "EPOCH: 24 step: 9350 VAL LOSS: 1.2684333324432373\n",
      "EPOCH: 24 step: 9375 VAL LOSS: 1.2666105031967163\n",
      "EPOCH: 24 step: 9400 VAL LOSS: 1.2701961994171143\n",
      "EPOCH: 24 step: 9425 VAL LOSS: 1.2668815851211548\n",
      "EPOCH: 24 step: 9450 VAL LOSS: 1.2681608200073242\n",
      "EPOCH: 24 step: 9475 VAL LOSS: 1.2678101062774658\n",
      "EPOCH: 24 step: 9500 VAL LOSS: 1.2644346952438354\n",
      "EPOCH: 24 step: 9525 VAL LOSS: 1.2683992385864258\n",
      "EPOCH: 24 step: 9550 VAL LOSS: 1.267024278640747\n",
      "\n",
      "Duration: 4253 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "model.train()\n",
    "\n",
    "if model.use_gpu:\n",
    "    model.cuda()\n",
    "    \n",
    "for i in range(epochs):\n",
    "    \n",
    "    hidden = model.hidden_state(batch_size)\n",
    "    \n",
    "    for x,y in generate_batches(train_data, batch_size, seq_len):\n",
    "    \n",
    "        tracker += 1\n",
    "        \n",
    "        x = one_hot_encoder(x, num_char)\n",
    "        \n",
    "        inputs = torch.from_numpy(x)\n",
    "        targets = torch.from_numpy(y)\n",
    "        \n",
    "        if model.use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        \n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        model.zero_grad()\n",
    "            \n",
    "        lstm_output, hidden = model.forward(inputs, hidden)\n",
    "        loss = criterion(lstm_output, targets.view(batch_size*seq_len).long())\n",
    "        loss.backward()\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if tracker % 25 == 0:\n",
    "            \n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            \n",
    "            for x,y in generate_batches(val_data, batch_size, seq_len):\n",
    "                \n",
    "                x = one_hot_encoder(x, num_char)\n",
    "        \n",
    "                inputs = torch.from_numpy(x)\n",
    "                targets = torch.from_numpy(y)\n",
    "\n",
    "                if model.use_gpu:\n",
    "                    inputs = inputs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "                \n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "                \n",
    "                lstm_output, val_hidden = model.forward(inputs, val_hidden)\n",
    "                val_loss = criterion(lstm_output, targets.view(batch_size*seq_len).long())\n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            model.train()\n",
    "            \n",
    "            print(f\"EPOCH: {i} step: {tracker} VAL LOSS: {val_loss.item()}\")\n",
    "\n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"hidden512_layers3_shakes_saul_4.net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
